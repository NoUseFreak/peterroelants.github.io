{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement a neural network: Intermezzo 1\n",
    "\n",
    "This is the first intermezzo of a 4 (+2) parts tutorial on how to implement a simple neural network model. You can find the links to the rest of the tutorial here:\n",
    "\n",
    "* [Part 1: Linear regression]({% post_url 2015-06-10-neural_network_implementation_part01 %})\n",
    "* [Intermezzo 1: Logistic classification function]({% post_url 2015-06-10-neural_network_implementation_intermezzo01 %})\n",
    "* [Part 2: Logistic regression (classification)]({% post_url 2015-06-10-neural_network_implementation_part02 %})\n",
    "* [Part 3: Hidden layer]({% post_url 2015-06-10-neural_network_implementation_part03 %})\n",
    "* [Intermezzo 2: Softmax classification function]({% post_url 2015-06-10-neural_network_implementation_intermezzo02 %})\n",
    "* [Part 4: Vectorization]({% post_url 2015-06-10-neural_network_implementation_part04 %})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic classification function\n",
    "\n",
    "If we want to do classification with neural networks we want to output a probability distribution over the classes from the output targets $t$. For the classification of 2 classes $t=1$ or $t=0$ we can use the [logistic function](http://en.wikipedia.org/wiki/Logistic_function) used in [logistic regression](http://en.wikipedia.org/wiki/Logistic_regression). For multiclass classification there exists an extension of this logistic function called the [softmax function](http://en.wikipedia.org/wiki/Softmax_function) which is used in [multinomial logistic regression](http://en.wikipedia.org/wiki/Multinomial_logistic_regression). The following section will explain the logistic function and how to optimize it, the [next intermezzo]({% post_url 2015-06-10-neural_network_implementation_intermezzo02 %}) will explain the softmax function and how to derive it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function\n",
    "\n",
    "The goal is to predict the target class $t$ from an input $z$. The probability $P(t=1 | z)$ that input $z$ is classified as class $t=1$ is represented by the output $y$ of the logistic function computed as $y = \\sigma(z)$. $\\sigma$ is the [logistic function](http://en.wikipedia.org/wiki/Logistic_function) and is defined as:\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
    "\n",
    "This logistic function, implemented below as `logistic(z)`, maps the input $z$ to an output between $0$ and $1$ as is illustrated in the figure below.\n",
    "\n",
    "We can write the probabilities that the class is $t=1$ or $t=0$ given input $z$ as:\n",
    "\n",
    "$$\\begin{split}\n",
    "P(t=1| z) & = \\sigma(z) = \\frac{1}{1+e^{-z}} \\\\\n",
    "P(t=0| z) & = 1 - \\sigma(z) = \\frac{e^{-z}}{1+e^{-z}} \n",
    "\\end{split}$$\n",
    "\n",
    "Note that input $z$ to the logistic function corresponds to the log [odds ratio](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm) of $P(t=1|z)$ over $P(t=0|z)$.\n",
    "\n",
    "$$\\begin{split}\n",
    "log \\frac{P(t=1|z)}{P(t=0|z)} & = log \\frac{\\frac{1}{1+e^{-z}}}{\\frac{e^{-z}}{1+e^{-z}}} = log \\frac{1}{e^{-z}} \\\\\n",
    "& = log(1) - log(e^{-z}) = z\n",
    "\\end{split}$$\n",
    "\n",
    "This means that the logg odds ratio $log(P(t=1|z)/P(t=0|z))$ changes linearly with $z$. And if $z = x*w$ as in neural networks, this means that  the logg odds ratio changes linearly with the parameters $w$ and input samples $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Plot the logistic function\n",
    "z = np.linspace(-6,6,100)\n",
    "plt.plot(z, logistic(z), 'b-')\n",
    "plt.xlabel('$z$', fontsize=15)\n",
    "plt.ylabel('$\\sigma(z)$', fontsize=15)\n",
    "plt.title('logistic function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of the logistic function\n",
    "\n",
    "Since neural networks typically use [gradient](http://en.wikipedia.org/wiki/Gradient) based opimization techniques such as [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) it is important to define the [derivative](http://en.wikipedia.org/wiki/Derivative) of the output $y$ of the logistic function with respect to its input $z$. ${\\partial y}/{\\partial z}$ can be calculated as:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial z} = \\frac{\\partial \\sigma(z)}{\\partial z} = \\frac{\\partial \\frac{1}{1+e^{-z}}}{\\partial z} = \\frac{-1}{(1+e^{-z})^2} *e^{-z}*-1 = \\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}}$$\n",
    "\n",
    "And since $1 - \\sigma(z)) = 1 - {1}/(1+e^{-z}) = {e^{-z}}/(1+e^{-z})$ this can be rewritten as:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial z} = \\frac{1}{1+e^{-z}} \\frac{e^{-z}}{1+e^{-z}} = \\sigma(z) * (1- \\sigma(z)) =  y (1-y)$$\n",
    "\n",
    "This derivative is implemented as `logistic_derivative(z)` and is plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the logistic function\n",
    "def logistic_derivative(z): return logistic(z) * (1 - logistic(z))\n",
    "\n",
    "# Plot the logistic function\n",
    "z = np.linspace(-6,6,100)\n",
    "plt.plot(z, logistic_derivative(z), 'r-')\n",
    "plt.xlabel('$z$', fontsize=15)\n",
    "plt.ylabel('$\\\\frac{\\\\partial \\\\sigma(z)}{\\\\partial z}$', fontsize=15)\n",
    "plt.title('derivative of the logistic function')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy cost function for the logistic function\n",
    "\n",
    "The output of the model $y = \\sigma(z)$ can be interpreted as a probability $y$ that input $z$ belongs to one class $(t=1)$, or probability $1-y$ that $z$ belongs to the other class $(t=0)$ in a two class classification problem. We note this down as: $P(t=1| z) = \\sigma(z) = y$.\n",
    "\n",
    "The neural network model will be optimized by maximizing the [likelihood](http://en.wikipedia.org/wiki/Likelihood_function) that a given set of parameters $\\theta$ of the model can result in a prediction of the correct class of each input sample. The parameters $\\theta$ transform each input sample $i$ into an input to the logistic function $z_{i}$. The likelihood maximization can be written as:\n",
    "\n",
    "$$\\underset{\\theta}{\\text{argmax}}\\; \\mathcal{L}(\\theta|t,z) = \\underset{\\theta}{\\text{argmax}} \\prod_{i=1}^{n} \\mathcal{L}(\\theta|t_i,z_i)$$\n",
    "\n",
    "The likelihood $\\mathcal{L}(\\theta|t,z)$ can be rewritten as the [joint probability](http://en.wikipedia.org/wiki/Joint_probability_distribution) of generating $t$ and $z$ given the parameters $\\theta$: $P(t,z|\\theta)$. Since $P(A,B) = P(A|B)*P(B)$ this can be written as:\n",
    "\n",
    "$$P(t,z|\\theta) = P(t|z,\\theta)P(z|\\theta)$$\n",
    "\n",
    "Since we are not interested in the probability of $z$ we can reduce this to: $\\mathcal{L}(\\theta|t,z) = P(t|z,\\theta) = \\prod_{i=1}^{n} P(t_i|z_i,\\theta)$. \n",
    "Since $t_i$ is a [Bernoulli variable](http://en.wikipedia.org/wiki/Bernoulli_distribution), and the probability $P(t| z) = y$ is fixed for a given $\\theta$ we can rewrite this as: \n",
    "\n",
    "$$\\begin{split}\n",
    "P(t|z) & = \\prod_{i=1}^{n} P(t_i=1|z_i)^{t_i} * (1 - P(t_i=1|z_i))^{1-t_i} \\\\\n",
    "& = \\prod_{i=1}^{n} y_i^{t_i} * (1 - y_i)^{1-t_i} \\end{split}$$\n",
    "\n",
    "Since the logarithmic function is a monotone increasing function we can optimize the log-likelihood function $\\underset{\\theta}{\\text{argmax}}\\; log \\mathcal{L}(\\theta|t,z)$. This maximum will be the same as the maximum from the regular likelihood function. The log-likelihood function can be written as:\n",
    "\n",
    "$$\\begin{split} log \\mathcal{L}(\\theta|t,z) & = log \\prod_{i=1}^{n} y_i^{t_i} * (1 - y_i)^{1-t_i} \\\\\n",
    "& = \\sum_{i=1}^{n} t_i log(y_i) + (1-t_i) log(1 - y_i)\n",
    "\\end{split}$$\n",
    "\n",
    "Minimizing the negative of this function (minimizing the negative log likelihood) corresponds to maximizing the likelihood. This error function $\\xi(t,y)$ is typically known as the [cross-entropy error function](http://en.wikipedia.org/wiki/Cross_entropy) (also known as log-loss):\n",
    "\n",
    "$$\\begin{split}\n",
    "\\xi(t,y) & = - log \\mathcal{L}(\\theta|t,z) \\\\\n",
    "& = - \\sum_{i=1}^{n} \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right] \\\\\n",
    "& = - \\sum_{i=1}^{n} \\left[ t_i log(\\sigma(z) + (1-t_i)log(1-\\sigma(z)) \\right]\n",
    "\\end{split}$$\n",
    "\n",
    "This function looks complicated but besides the previous derivation there are a couple of intuitions why this function is used as a cost function for logistic regression. First of all it can be rewritten as:\n",
    "\n",
    "$$ \\xi(t_i,y_i) = \n",
    "   \\begin{cases}\n",
    "   -log(y_i) & \\text{if } t_i = 1 \\\\\n",
    "   -log(1-y_i) & \\text{if } t_i = 0\n",
    "  \\end{cases}$$\n",
    "  \n",
    "Which in the case of $t_i=1$ is $0$ if $y_i=1$ $(-log(1)=0)$ and goes to infinity as $y_i \\rightarrow 0$ $(\\underset{y \\rightarrow 0}{\\text{lim}}  -log(y) = +\\infty)$. The reverse effect is happening if $t_i=0$.  \n",
    "So what we end up with is a cost function that is $0$ if the probability to predict the correct class is $1$ and goes to infinity as the probability to predict the correct class goes to $0$.\n",
    "\n",
    "Notice that the cost function $\\xi(t,y)$ is equal to the negative [log probability](http://en.wikipedia.org/wiki/Log_probability) that $z$ is classified as its correct class:  \n",
    "$-log(P(t=1| z)) = -log(y)$,  \n",
    "$-log(P(t=0| z)) = -log(1-y)$.\n",
    "\n",
    "By minimizing the negative log probability, we will maximize the log probability. And since $t$ can only be $0$ or $1$, we can write $\\xi(t,y)$ as:\n",
    "$$ \\xi(t,y) = -t * log(y) - (1-t) * log(1-y) $$\n",
    "\n",
    "Which will give $\\xi(t,y) = - \\sum_{i=1}^{n} \\left[ t_i log(y_i) + (1-t_i)log(1-y_i) \\right]$ if we sum over all $n$ samples.\n",
    "\n",
    "\n",
    "Another reason to use the cross-entropy function is that in simple logistic regression this results in a [convex](http://en.wikipedia.org/wiki/Convex_function) cost function, of which the global minimum will be easy to find. Note that this is not necessarily the case anymore in multilayer neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative of the cross-entropy cost function for the logistic function\n",
    "\n",
    "The derivative ${\\partial \\xi}/{\\partial y}$ of the cost function with respect to its input can be calculated as:\n",
    "\n",
    "$$\\begin{split}\n",
    "\\frac{\\partial \\xi}{\\partial y} & = \\frac{\\partial (-t * log(y) - (1-t)* log(1-y))}{\\partial y} = \\frac{\\partial (-t * log(y))}{\\partial y} +  \\frac{\\partial (- (1-t)*log(1-y))}{\\partial y} \\\\\n",
    "& = -\\frac{t}{y} + \\frac{1-t}{1-y} = \\frac{y-t}{y(1-y)}\n",
    "\\end{split}$$\n",
    "\n",
    "This derivative will give a nice formula if it is used to calculate the derivative of the cost function with respect to the inputs of the classifier ${\\partial \\xi}/{\\partial z}$ since the derivative of the logistic function is ${\\partial y}/{\\partial z} =  y (1-y)$:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial z} = \\frac{\\partial y}{\\partial z} \\frac{\\partial \\xi}{\\partial y} = y (1-y) \\frac{y-t}{y(1-y)} = y-t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is generated from an IPython notebook file. [Link to the full IPython notebook file](https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/neural_net_implementation/neural_network_implementation_intermezzo01.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
